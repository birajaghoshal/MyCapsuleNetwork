{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demo is a implementation of capsule network according to Dynamic Routing Between Capsules(https://arxiv.org/pdf/1710.09829.pdf), the main ideas:\n",
    "- Use capsule vectors to replace scalars of each layer, for example, in a conventional neuron, the input are scalars whereas in a capsule, the input are capsule vectors.\n",
    "- Use coupling coefficients to replace weights between two layers. The connection between neurons of previous layer and neurons of current layer are replaced by dynamic routing algorithms.\n",
    "- Use the vector length to show how likely it exists. \n",
    "\n",
    "The code is referred to github.com/naturomics and my work includes:\n",
    "- Map the object of capsule network into an explicit procedures of implementing capsule model.\n",
    "- Add comments based on my personal understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#Import required packages\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "#Load MNIST dataset\n",
    "import tensorflow.examples.tutorials.mnist.input_data as input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55000\n"
     ]
    }
   ],
   "source": [
    "print(len(mnist.train.images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(mnist.test.images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define a squash function to get the length of a vector which stands for the existence probability, a vector which has a large norm value means high probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "epsilon = 1e-9\n",
    "#This function referred to https://github.com/naturomics/CapsNet-Tensorflow\n",
    "def squash(vector):\n",
    "\n",
    "    '''Squashing function corresponding to Eq. 1\n",
    "\n",
    "    Args:\n",
    "\n",
    "        vector: A tensor with shape [batch_size, 1, num_caps, vec_len, 1] or [batch_size, num_caps, vec_len, 1].\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        A tensor with the same shape as vector but squashed in 'vec_len' dimension.\n",
    "\n",
    "    '''\n",
    "\n",
    "    vec_squared_norm = tf.reduce_sum(tf.square(vector), -2, keep_dims=True)\n",
    "\n",
    "    scalar_factor = vec_squared_norm / (1 + vec_squared_norm) / tf.sqrt(vec_squared_norm + epsilon)\n",
    "\n",
    "    vec_squashed = scalar_factor * vector  # element-wise\n",
    "\n",
    "    return(vec_squashed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the dynamic routing algorithm, it is very different from conventional neural network, in previous neural network models, the values of the next layers are determined by the output of previous layers and the weights between these two layers, the weights are trainable parameters. **However, in a capsule network, the relationship between two layers are determined by dynamic routing mechanism, the coefficients are calculated by iterations.** Actually, according to this [blog](http://www.sohu.com/a/218609089_500659), the dynamic routing is a way of finding clustering center for previous capsules. The graph is referred to http://www.sohu.com/a/218609089_500659.\n",
    "![dynamic_routing](http://5b0988e595225.cdn.sohucs.com/images/20180124/7fa017c7daf94e309971d80f1f1e3832.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, the affline mapping weights are shared across capsules of one layer like this way:\n",
    "![dynamic](http://5b0988e595225.cdn.sohucs.com/images/20180124/fa268f22122f44518f10cd831cc7c04c.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function referred to https://github.com/naturomics/CapsNet-Tensorflow\n",
    "def routing(input, b_IJ, iter_routing=10):\n",
    "\n",
    "    ''' The routing algorithm.\n",
    "    Args:\n",
    "\n",
    "        input: A Tensor with [batch_size, num_caps_l=1152, 1, length(u_i)=8, 1]\n",
    "\n",
    "               shape, num_caps_l meaning the number of capsule in the layer l.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        A Tensor of shape [batch_size, num_caps_l_plus_1, length(v_j)=16, 1]\n",
    "\n",
    "        representing the vector output `v_j` in the layer l+1\n",
    "\n",
    "    Notes:\n",
    "\n",
    "        u_i represents the vector output of capsule i in the layer l, and\n",
    "\n",
    "        v_j the vector output of capsule j in the layer l+1.\n",
    "\n",
    "     '''\n",
    "    # W: [1, num_caps_i, num_caps_j * len_v_j, len_u_j, 1]\n",
    "    #Note, these weights are shared across different capsules within one layer\n",
    "\n",
    "    W = tf.get_variable('Weight', shape=(1, 1152, 160, 8, 1), dtype=tf.float32,\n",
    "\n",
    "                        initializer=tf.random_normal_initializer(0.01))\n",
    "\n",
    "    biases = tf.get_variable('bias', shape=(1, 1, 10, 16, 1))\n",
    "\n",
    "    # Eq.2, calc u_hat\n",
    "\n",
    "    # Since tf.matmul is a time-consuming op,\n",
    "\n",
    "    # A better solution is using element-wise multiply, reduce_sum and reshape\n",
    "\n",
    "    # ops instead. Matmul [a, b] x [b, c] is equal to a series ops as\n",
    "\n",
    "    # element-wise multiply [a*c, b] * [a*c, b], reduce_sum at axis=1 and\n",
    "\n",
    "    # reshape to [a, c]\n",
    "\n",
    "    input = tf.tile(input, [1, 1, 160, 1, 1])\n",
    "\n",
    "    assert input.get_shape() == [batch_size, 1152, 160, 8, 1]\n",
    "\n",
    "    u_hat = tf.reduce_sum(W * input, axis=3, keep_dims=True)\n",
    "\n",
    "    u_hat = tf.reshape(u_hat, shape=[-1, 1152, 10, 16, 1])\n",
    "\n",
    "    assert u_hat.get_shape() == [batch_size, 1152, 10, 16, 1]\n",
    "\n",
    "\n",
    "    # In forward, u_hat_stopped = u_hat; in backward, no gradient passed back from u_hat_stopped to u_hat\n",
    "    u_hat_stopped = tf.stop_gradient(u_hat, name='stop_gradient')\n",
    "\n",
    "    # line 3,for r iterations do\n",
    "    for r_iter in range(iter_routing):\n",
    "\n",
    "        with tf.variable_scope('iter_' + str(r_iter)):\n",
    "\n",
    "            # line 4:\n",
    "\n",
    "            # => [batch_size, 1152, 10, 1, 1]\n",
    "            #print(b_IJ)\n",
    "\n",
    "            c_IJ = tf.nn.softmax(b_IJ, dim=2)\n",
    "\n",
    "\n",
    "\n",
    "            # At last iteration, use `u_hat` in order to receive gradients from the following graph\n",
    "\n",
    "            if r_iter == iter_routing - 1:\n",
    "\n",
    "                # line 5:\n",
    "\n",
    "                # weighting u_hat with c_IJ, element-wise in the last two dims\n",
    "\n",
    "                # => [batch_size, 1152, 10, 16, 1]\n",
    "\n",
    "                s_J = tf.multiply(c_IJ, u_hat)\n",
    "\n",
    "                # then sum in the second dim, resulting in [batch_size, 1, 10, 16, 1]\n",
    "\n",
    "                s_J = tf.reduce_sum(s_J, axis=1, keep_dims=True) + biases\n",
    "\n",
    "                assert s_J.get_shape() == [batch_size, 1, 10, 16, 1]\n",
    "\n",
    "\n",
    "\n",
    "                # line 6:\n",
    "\n",
    "                # squash using Eq.1,\n",
    "\n",
    "                v_J = squash(s_J)\n",
    "\n",
    "                assert v_J.get_shape() == [batch_size, 1, 10, 16, 1]\n",
    "\n",
    "            elif r_iter < iter_routing - 1:  # Inner iterations, do not apply backpropagation\n",
    "\n",
    "                s_J = tf.multiply(c_IJ, u_hat_stopped)\n",
    "\n",
    "                s_J = tf.reduce_sum(s_J, axis=1, keep_dims=True) + biases\n",
    "\n",
    "                v_J = squash(s_J)\n",
    "\n",
    "\n",
    "                # line 7:\n",
    "\n",
    "                # reshape & tile v_j from [batch_size ,1, 10, 16, 1] to [batch_size, 1152, 10, 16, 1]\n",
    "\n",
    "                # then matmul in the last tow dim: [16, 1].T x [16, 1] => [1, 1], reduce mean in the\n",
    "\n",
    "                # batch_size dim, resulting in [1, 1152, 10, 1, 1]\n",
    "\n",
    "                v_J_tiled = tf.tile(v_J, [1, 1152, 1, 1, 1])\n",
    "\n",
    "                u_produce_v = tf.reduce_sum(u_hat_stopped * v_J_tiled, axis=3, keep_dims=True)\n",
    "\n",
    "                assert u_produce_v.get_shape() == [batch_size, 1152, 10, 1, 1]\n",
    "\n",
    "\n",
    "                # b_IJ += tf.reduce_sum(u_produce_v, axis=0, keep_dims=True)\n",
    "\n",
    "                b_IJ += u_produce_v\n",
    "\n",
    "\n",
    "\n",
    "    return(v_J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note, there are no activation functions and pooling methods, I think this capsule model tries to make full use of information instead of filtering them out.** In this example there are three layers in sequence:\n",
    "- 1 A 9X9X256 conv filter to get image features 20X20X256\n",
    "- 2 A group(8) of 9X9X32 conv filter to get 1152 capsules with lengths of 8, squashing function is implemented for the output\n",
    "- 3 10 capsules with lengths of 16\n",
    "\n",
    "There are 10 capsules in the output, each has a length of 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"routing/Reshape:0\", shape=(50, 1152, 1, 8, 1), dtype=float32)\n",
      "Tensor(\"Squeeze:0\", shape=(50, 10, 16, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "graph_capsule = tf.Graph()\n",
    "with graph_capsule.as_default() as g:\n",
    "    #Create input placeholders\n",
    "    x = tf.placeholder(tf.float32, [batch_size, 784])\n",
    "    y_ = tf.placeholder(tf.float32, [batch_size,10])\n",
    "    #Reshape\n",
    "    x_image = tf.reshape(x, [-1,28,28,1])\n",
    "    #Use a convnet to extract local features as input for capsules\n",
    "    with tf.name_scope('conv_input'):\n",
    "        #batch_size, 20, 20, 256\n",
    "        input_data = tf.contrib.layers.conv2d(inputs=x_image, num_outputs=256, \n",
    "                                              kernel_size=9, padding='valid')\n",
    "        \n",
    "    #PrimaryCaps\n",
    "    ######################Create first capsule layer#################################\n",
    "    capsules = []\n",
    "    #The vector length of the capsule is 8\n",
    "    #Create 8 conv outputs, stacks of 8 scales\n",
    "    for i in range(8):\n",
    "        with tf.name_scope('cap_conv_' + str(i)):\n",
    "            #output: batch_size, 6, 6, 32\n",
    "            output = tf.contrib.layers.conv2d(inputs=input_data, num_outputs=32, \n",
    "                                              kernel_size=9, stride=2, padding='valid')\n",
    "            output = tf.reshape(output, [batch_size, -1, 1, 1])\n",
    "            capsules.append(output)\n",
    "    #Concatenate the outputs to form capsule vectors\n",
    "    #1152 capsule vectors of 8-length for each image\n",
    "    capsules_0 = tf.concat(capsules, axis=2)\n",
    "    #remove the last 1 dim, batch_size*1152*8\n",
    "    #Normalize the vectors by squashing function\n",
    "    capsules_0 = squash(capsules_0)\n",
    "    \n",
    "    \n",
    "    #####################Create second capsule layer########################################\n",
    "    #print(capsules_0)\n",
    "    #Routing,DIgitCaps\n",
    "    #Output 10 capsules of 16-length\n",
    "    with tf.name_scope('routing'):\n",
    "        #Batch_size*1152*1*8*1, extend dimension? Not quite sure the reasons behind\n",
    "        outputs = tf.reshape(capsules_0, shape=(batch_size, -1, 1, \n",
    "                                              capsules_0.shape[-2].value, 1))\n",
    "        print(outputs)\n",
    "        #Initializing coefficients\n",
    "        b_IJ = tf.constant(np.zeros([1, capsules_0.shape[1].value, 10, 1, 1], dtype=np.float32))\n",
    "        capsules_1 = routing(outputs, b_IJ)\n",
    "    #10 capsules, each capsule consist of 16-length\n",
    "    capsules_1 = tf.squeeze(capsules_1, axis=1)\n",
    "    print(capsules_1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unlike previous CNNs, RNNs, where we use fully-connected network to map features into a output layer whose neurons corresponding to the classes, here we use the norms of each capsule to represent the probabilty of its existence.** The number of capsules is corresponding the number of classes, and the capsule which has the longest norm stands for the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Masking_1/Sum_1:0\", shape=(), dtype=float32)\n",
      "Tensor(\"Masking_1/Mul:0\", shape=(50, 10, 16), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "epsilon = 1e-9\n",
    "with graph_capsule.as_default() as g:\n",
    "    with tf.variable_scope('Masking'):\n",
    "        #Calculate the norms of each capsule\n",
    "        v_length = tf.sqrt(tf.reduce_sum(tf.square(capsules_1),\n",
    "                                           axis=2, keep_dims=True) + epsilon)\n",
    "        #Use softmax to get prediction probabilities\n",
    "        softmax_v = tf.nn.softmax(v_length, dim=1)\n",
    "        assert softmax_v.get_shape() == [batch_size, 10, 1, 1]\n",
    "\n",
    "        # b). pick out the index of max softmax val of the 10 caps\n",
    "        # [batch_size, 10, 1, 1] => [batch_size] (index)\n",
    "        argmax_idx = tf.to_int32(tf.argmax(softmax_v, axis=1))\n",
    "        assert argmax_idx.get_shape() == [batch_size, 1, 1]\n",
    "        \n",
    "        argmax_idx = tf.reshape(argmax_idx, shape=(batch_size, ))\n",
    "        #Calculate accuracy\n",
    "        label_idx = tf.to_int32(tf.argmax(y_, axis=1))\n",
    "        correct_bool = tf.cast(tf.equal(argmax_idx, label_idx), tf.float32)\n",
    "        accuracy = tf.reduce_mean(correct_bool)\n",
    "        correct_num = tf.reduce_sum(correct_bool)\n",
    "        #print(correct_num)\n",
    "        \n",
    "        #Select the corresponding capsule of 16-length\n",
    "        masked_v = tf.multiply(tf.squeeze(capsules_1), tf.reshape(y_, (-1, 10, 1)))\n",
    "        print(masked_v)\n",
    "        #v_length = tf.sqrt(tf.reduce_sum(tf.square(capsules_1), axis=2, keep_dims=True) + epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In order to validate how well the capsules can retain the original information, in the paper, Hinton tried to reconstruct the original images based on the output capsules.** There three fully-connected layers:\n",
    "- A layer 160X512\n",
    "- A layer 512X1024\n",
    "- A layer 1024X784\n",
    "\n",
    "For this part, it is a regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Decoder_1/Reshape:0\", shape=(50, 160), dtype=float32)\n",
      "Tensor(\"Decoder_1/fully_connected_2/Sigmoid:0\", shape=(50, 784), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with graph_capsule.as_default() as g:\n",
    "    ###########Decoder, to reconstruct the digits#############\n",
    "    with tf.name_scope('Decoder'):\n",
    "        #Use the corresponding capsule vector to reconstruct the digit\n",
    "        #Use two fully-connected layers\n",
    "        vector_j = tf.reshape(masked_v, shape=(batch_size, -1))\n",
    "        print(vector_j)\n",
    "        fc1 = tf.contrib.layers.fully_connected(vector_j, num_outputs=512)\n",
    "        assert fc1.get_shape() == [batch_size, 512]\n",
    "           \n",
    "        fc2 = tf.contrib.layers.fully_connected(fc1, num_outputs=1024)\n",
    "        assert fc2.get_shape() == [batch_size, 1024]\n",
    "\n",
    "        decoded = tf.contrib.layers.fully_connected(fc2, num_outputs=784, activation_fn=tf.sigmoid)\n",
    "        print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now it's time for loss functions, unlike previous cross-entropy loss, here a marginal loss is adopted, which includes two parts**:\n",
    "- The loss for the longest capsule, the norm needs to be as large as 0.9\n",
    "- The loss for the shorter capsules, the norm needs to be smaller than 0.1\n",
    "The purpose of the loss function is to make the dominant capsule as strong as possible whereas the other capsules less important.\n",
    "\n",
    "In addtion, a regularization loss which means the construction error is also considered. The loss is Mean Squared Error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_3:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#Define Loss\n",
    "lambda_value = 0.5\n",
    "regularization_scale = 0.0005\n",
    "with graph_capsule.as_default() as g:\n",
    "    #####################Define Margin Loss ##################\n",
    "    #The loss for the prediction of dominant capsule\n",
    "    max_l = tf.square(tf.maximum(0.,0.9 - v_length))\n",
    "    #The loss for prediction of other capsule\n",
    "    max_r = tf.square(tf.maximum(0., v_length - 0.1))\n",
    "    assert max_l.get_shape() == [batch_size, 10, 1, 1]\n",
    "\n",
    "    # reshape: [batch_size, 10, 1, 1] => [batch_size, 10]\n",
    "    max_l = tf.reshape(max_l, shape=(batch_size, -1))\n",
    "    max_r = tf.reshape(max_r, shape=(batch_size, -1))\n",
    "    # calc T_c: [batch_size, 10]\n",
    "    # T_c = Y, is my understanding correct? Try it.\n",
    "    T_c = y_\n",
    "    # [batch_size, 10], element-wise multiply\n",
    "    L_c = T_c * max_l + lambda_value * (1 - T_c) * max_r\n",
    "    margin_loss = tf.reduce_mean(tf.reduce_sum(L_c, axis=1))\n",
    "    \n",
    "    ####################Define Reconstruction Loss############\n",
    "    #mean square error for the regression\n",
    "    orgin = tf.reshape(x, shape=(batch_size, -1))\n",
    "    squared = tf.square(decoded - orgin)\n",
    "    reconstruction_err = tf.reduce_mean(squared)\n",
    "    # Total loss\n",
    "    total_loss = margin_loss + regularization_scale * reconstruction_err\n",
    "    print(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define Optimization\n",
    "with graph_capsule.as_default() as g:\n",
    "    optimizer = tf.train.AdamOptimizer(0.0001)\n",
    "    train_op = optimizer.minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.555879 0.04\n",
      "Loss: 0.0449784 0.96\n",
      "Loss: 0.0336064 1.0\n",
      "Loss: 0.0103637 1.0\n",
      "Loss: 0.0118929 1.0\n",
      "Loss: 0.00713452 1.0\n",
      "Loss: 0.00686674 1.0\n",
      "Loss: 0.0133592 1.0\n",
      "Loss: 0.00595833 1.0\n",
      "Testing Accuracy:  0.9866\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "num_steps = int(len(mnist.train.images)/batch_size)\n",
    "with tf.Session(graph=graph_capsule) as sess:\n",
    "    #Initialize variables\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    for _ in range(epochs):\n",
    "        for step in range(num_steps):\n",
    "            batch_data, batch_labels = mnist.train.next_batch(batch_size)\n",
    "            feed_dict = {x : batch_data, y_ : batch_labels}\n",
    "            #Train\n",
    "            _, loss, acc = sess.run([train_op, total_loss, accuracy], feed_dict=feed_dict)\n",
    "            if step%500 == 0:\n",
    "                print('Loss:', loss, acc)\n",
    "     \n",
    "    loops = int(len(mnist.test.images)/batch_size)\n",
    "    count = 0\n",
    "    for i in range(loops):\n",
    "        batch_data, batch_labels = mnist.test.next_batch(batch_size)\n",
    "        feed_dict = {x : batch_data, y_ : batch_labels}\n",
    "        #Train\n",
    "        num = sess.run(correct_num, feed_dict=feed_dict)\n",
    "        count += num\n",
    "    print('Testing Accuracy: ', count*1.0/10000)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# References\n",
    "1. Dynamic Routing Between Capsules, Sara Sabour and etc, NIPS 2017.\n",
    "2. 揭开迷雾，来一顿美味的「Capsule」盛宴, 苏剑林,  http://www.sohu.com/a/218609089_500659 .\n",
    "3. 从K-Means到Capsule, 苏剑林, https://kexue.fm/archives/5112\n",
    "4. Capsule Tensorflow Implementation, Naturomics, https://github.com/naturomics/CapsNet-Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
